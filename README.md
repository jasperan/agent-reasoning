# Agent Reasoning: The Thinking Layer

![License](https://img.shields.io/badge/license-MIT-green)
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![Ollama](https://img.shields.io/badge/backend-Ollama-black)
![Reasoning](https://img.shields.io/badge/reasoning-CoT%20|%20ToT%20|%20ReAct-purple)
![Status](https://img.shields.io/badge/status-experimental-orange)

## Vision & Purpose

The **Reasoning Layer** is the cognitive engine of the AI stack. While traditional LLMs excel at token generation, they often struggle with complex planning, logical deduction, and self-correction.

This repository transforms standard Open Source models (like `gemma3`, `llama3`) into robust problem solvers by wrapping them in advanced cognitive architectures. It implements findings from key research papers (CoT, ToT, ReAct) to give models "agency" over their thinking process.

> **"From predicting the next token to predicting the next thought."**

---

## ðŸš€ Features

*   **Plug & Play**: Use via Python Class or as a Network Proxy.
*   **Model Agnostic**: Works with any model served by Ollama (tested on `gemma3:270m` for speed/demo).
*   **Advanced Architectures**:
    *   ðŸ”— **Chain-of-Thought (CoT)**
    *   ðŸŒ³ **Tree of Thoughts (ToT)**
    *   ðŸ› ï¸ **ReAct (Reason + Act)**
    *   ðŸªž **Self-Reflection**
    *   ðŸ§© **Decomposition & Least-to-Most**

---

## ðŸ“¦ Installation

```bash
# Clone the repo
git clone https://github.com/your-username/agent-reasoning.git
cd agent-reasoning

# Install dependencies
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# (Optional) Install as editable package
pip install -e .
```

**Prerequisite**: [Ollama](https://ollama.com/) must be running.
```bash
ollama pull gemma3:270m
```

---

## ðŸ’» Usage

### 1. The Benchmark (Quick Start)
Run the built-in benchmark to see the agents reason through Logic, Philosophy, and Planning tasks.

```bash
python main.py
```

### 2. Python Interceptor (For Developers)
Use the `ReasoningInterceptor` as a drop-in replacement for your LLM client.

```python
from src.interceptor import ReasoningInterceptor

client = ReasoningInterceptor()

# Append the strategy to the model name with a '+'
response = client.generate(
    model="gemma3:270m+tot", 
    prompt="I have a 3-gallon and 5-gallon jug. How do I measure 4 gallons?"
)
print(response["response"])
```

### 3. Reasoning Gateway (For Any App)
Run a proxy server that impersonates Ollama. This allows **any** Ollama-compatible app (LangChain, Web UIs) to gain reasoning capabilities without code changes.

```bash
# Start the Gateway on port 8080
python server.py
```

Then configure your app:
*   **Base URL**: `http://localhost:8080`
*   **Model**: `gemma3:270m+cot` (or `+tot`, `+react`, etc.)

**Example:**
```bash
curl http://localhost:8080/api/generate -d '{
  "model": "gemma3:270m+cot",
  "prompt": "Why is the sky blue?"
}'
```

---

## ðŸ§  Architectures in Detail

| Architecture | Description | Best For | Papers |
|--------------|-------------|----------|--------|
| **Chain-of-Thought** | Step-by-step reasoning prompt injection. | Math, Logic, Explanations | [Wei et al. (2022)](https://arxiv.org/abs/2201.11903) |
| **Self-Reflection** | Draft -> Critique -> Refine loop. | Creative Writing, High Accuracy | [Shinn et al. (2023)](https://arxiv.org/abs/2303.11366) |
| **ReAct** | Interleaves Reasoning and Tool Usage. | Fact-checking, Calculations | [Yao et al. (2022)](https://arxiv.org/abs/2210.03629) |
| **Tree of Thoughts** | Explores multiple reasoning branches (BFS/DFS). | Complex Riddles, Strategy | [Yao et al. (2023)](https://arxiv.org/abs/2305.10601) |
| **Decomposed** | Breaks complex queries into sub-tasks. | Planning, Long-form answers | [Khot et al. (2022)](https://arxiv.org/abs/2210.02406) |

---

## ðŸ“š Appendix A: Extending the System

To add a new reasoning strategy (e.g., "Reviewer-Critic"), simply:

1.  Create a class in `src/agents/` inheriting from `BaseAgent`.
2.  Implement the `stream(self, query)` method.
3.  Register it in `AGENT_MAP` in `src/interceptor.py` and `server.py`.

```python
class MyNewAgent(BaseAgent):
    def stream(self, query):
        yield "Thinking differently...\n"
        # ... your custom logic ...
        yield "Final Answer"
```

## ðŸ”§ Appendix B: Troubleshooting

*   **Model Not Found**: Ensure you have pulled the base model (`ollama pull gemma3:270m`).
*   **Timeout / Slow**: ToT and Self-Reflection make multiple calls to the LLM. With larger models (Llama3 70b), this can take time.
*   **Hallucinations**: The default demo uses `gemma3:270m` which is extremely small and prone to logic errors. Switch to `gemma2:9b` or `llama3` for robust results.

---


---

## ðŸ“Š Benchmark Report (Example Outputs)

Below are real outputs generated by the `main.py` benchmark using `gemma3:270m`. Note that while the small model strives to follow the reasoning structures, its logic limitations highlight the importance of using larger models (e.g., `llama3` or `gemma2:9b`) for production.

### 1. Philosophy (Standard Agent)
*Straightforward generation without structured thought.*

**Query:** "What is the meaning of life? Answer with a mix of biological and philosophical perspectives."
```text
[StandardAgent]: Processing query...
Answer: The meaning of life is a complex and multifaceted question that has been pondered by philosophers, theologians, and scientists for centuries. Biologically, life is the condition that distinguishes animals and plants from inorganic matter...
```

### 2. Planning (Decomposed Agent)
*Breaks down complex tasks into sub-problems.*

**Query:** "Plan a detailed 3-day itinerary for Tokyo..."
```text
[DecomposedAgent]: Decomposing the problem...

Sub-tasks Plan:
1.  **Define the topic:** What is the historical theme?
2.  **Identify the target audience:** History buff who loves samurais.
3.  **Determine the timeframe:** 3 days.
4.  **Identify key attractions:** Temples, museums.
...
[DecomposedAgent]: Solving sub-task: 1. Define the topic...
[DecomposedAgent]: Solving sub-task: 2. Identify the target audience...
Final Answer: Okay, I'm ready to help with your 3-day historical Tokyo itinerary!
```

### 3. Complex Physics (Least-to-Most Agent)
*Deconstructs hard problems into a chain of easier questions.*

**Query:** "A train travels at 0.6c for 5 years. How much time has passed on Earth?"
```text
[LeastToMostAgent]: Decomposing into sub-questions (easy -> hard)...
Plan:
1. Identify the Variable: Time elapsed on Earth.
2. Calculate the Distance: 0.6c * 5 years.
3. Calculate the Time Elapsed...

[LeastToMostAgent]: Addressing: 1. Identify the Variable...
Answer: Okay.
[LeastToMostAgent]: Addressing: 2. Calculate the Distance...
Answer: The distance is 3 light-years.
[LeastToMostAgent]: Addressing: 5. Determine the Time on Earth...
```

### 4. Tool Use (ReAct Agent)
*Interleaves thought, action, and observation to solve problems.*

**Query:** "Calculate the square root of 144..."
```text
[ReActAgent]: Processing query with ReAct...
--- Step 1 ---
Agent: Thought: I need to calculate the square root of 144.
Action: <calculate_square_root>
Observation: 12
Final Answer: 12 * 153 ...
```

